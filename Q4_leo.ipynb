{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q4_leo.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOW+R33ebxBE8o5yFfB+Kto"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mCcn-Wm2UYfB","colab_type":"text"},"source":["#Question 4"]},{"cell_type":"code","metadata":{"id":"h-uf-li5cy0T","colab_type":"code","colab":{}},"source":["import numpy as np\n","import numpy as np\n","import math\n","import timeit\n","\n","from tqdm import tqdm\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","\n","import random\n","random.seed(141)\n","np.random.seed(141)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpkY-ZNp0VGz","colab_type":"code","colab":{}},"source":["class MazeMDP2:\n","    def __init__(self):\n","  \n","        self.i_numStates = 16 #15 states since 1 and 16 are the terminal states\n","        # actions: 0=up, 1=down, 2=right, 3=left\n","        self.v_actions = [[1,2],[1,2,3],[1,2,3],[1,2,3],[0,1,2],[0,1,2,3],[0,1,2,3],[0,1,3],[0,1,3],[0,1,2,3],[0,1,2,3],[0,1,3],[0,2],[0,2,3],[0,2,3],[0,3]]  # eligible actions\n","\n","        flat_acList = [item for sublist in self.v_actions for item in sublist] # flatten the list of lists\n","        #flat_acList = [0,1,2,3]\n","        self.i_numActions = len(set(flat_acList)) # unique elements are obtained via set()\n","\n","        # transition probabilities\n","        self.v_trp = [[[0 for i in range(self.i_numStates)] for j in range(self.i_numStates)] for k in range(self.i_numActions)]\n","\n","        ########action 0 GO UP###########\n","        self.v_trp[0][0][0] = 1 #ACTION -> STATE -> STATE'\n","        self.v_trp[0][1][1] = 1 \n","        self.v_trp[0][2][2] = 1 \n","        self.v_trp[0][3][3] = 1 \n","        \n","        self.v_trp[0][4][0] = 1 \n","        self.v_trp[0][5][1] = 1\n","        self.v_trp[0][6][2] = 1 \n","        self.v_trp[0][7][3] = 1 \n","\n","        self.v_trp[0][8][4] = 1 \n","        self.v_trp[0][9][5] = 1 \n","        self.v_trp[0][10][6] = 1 \n","        self.v_trp[0][11][7] = 1\n","\n","        self.v_trp[0][12][8] = 1 \n","        self.v_trp[0][13][9] = 1 \n","        self.v_trp[0][14][10] = 1 \n","        self.v_trp[0][15][11] = 1 \n","        \n","        ########action 1 GO DOWN###########\n","        self.v_trp[1][0][4] = 1\n","        self.v_trp[1][1][5] = 1 \n","        self.v_trp[1][2][6] = 1 \n","        self.v_trp[1][3][7] = 1 \n","        \n","        self.v_trp[1][4][8] = 1 \n","        self.v_trp[1][5][9] = 1 \n","        self.v_trp[1][6][10] = 1 \n","        self.v_trp[1][7][11] = 1 \n","        \n","        self.v_trp[1][8][12] = 1 \n","        self.v_trp[1][9][13] = 1\n","        self.v_trp[1][10][14] = 1 \n","        self.v_trp[1][11][15] = 1\n","\n","        self.v_trp[1][12][12] = 1 \n","        self.v_trp[1][13][13] = 1 \n","        self.v_trp[1][14][14] = 1 \n","        self.v_trp[1][15][15] = 1 \n","\n","        ########action 2 GO RIGHT###########\n","        self.v_trp[2][0][1] = 1\n","        self.v_trp[2][1][2] = 1 \n","        self.v_trp[2][2][3] = 1 \n","        self.v_trp[2][3][3] = 1\n","        \n","        self.v_trp[2][4][5] = 1 \n","        self.v_trp[2][5][5] = 1 \n","        self.v_trp[2][6][7] = 1 \n","        self.v_trp[2][7][7] = 1 \n","        \n","        self.v_trp[2][8][9] = 1 \n","        self.v_trp[2][9][10] = 1\n","        self.v_trp[2][10][11] = 1 \n","        self.v_trp[2][11][11] = 1\n","\n","        self.v_trp[2][12][13] = 1 \n","        self.v_trp[2][13][14] = 1 \n","        self.v_trp[2][14][15] = 1 \n","        self.v_trp[2][15][15] = 1 \n","\n","        ########action 3 GO LEFT   ###########\n","        self.v_trp[3][0][0] = 1\n","        self.v_trp[3][1][0] = 1 \n","        self.v_trp[3][2][1] = 1 \n","        self.v_trp[3][3][2] = 1 \n","        \n","        self.v_trp[3][4][4] = 1 \n","        self.v_trp[3][5][4] = 1 \n","        self.v_trp[3][6][5] = 1 \n","        self.v_trp[3][7][6] = 1 \n","        \n","        self.v_trp[3][8][8] = 1 \n","        self.v_trp[3][9][8] = 1\n","        self.v_trp[3][10][9] = 1 \n","        self.v_trp[3][11][10] = 1\n","\n","        self.v_trp[3][12][12] = 1 \n","        self.v_trp[3][13][12] = 1 \n","        self.v_trp[3][14][13] = 1 \n","        self.v_trp[3][15][14] = 1 \n","\n","\n","        # rewards3D - eveyrthing is -1 unless you hit the 2 terminal nodes, which are 1,4 ,11, 14\n","        self.v_rews3D = [[[-1 for i in range(self.i_numStates)] for j in range(self.i_numStates)]for a in range(self.i_numActions)]\n","        \n","        self.v_rews3D[0][0][0] = 0 # GO UP FROM state 0\n","        self.v_rews3D[3][0][0] = 0 # GO LEFT FROM state 0\n","\n","        self.v_rews3D[0][4][0] = 0 # GO UP FROM state 4\n","\n","        self.v_rews3D[1][11][15] = 0 # GO DOWN FROM state 11\n","        self.v_rews3D[2][14][15] = 0 # GO RIGHT FROM state 14\n","        \n","        self.v_rews3D[3][15][15] = 0 # GO LEFT FROM state 15\n","        self.v_rews3D[1][15][15] = 0 #GO DOWN from state 15\n","\n","        \n","        # rewards - not cool to do this here\n","        self.v_rews = [[0 for i in range(self.i_numActions)] for j in range(self.i_numStates)]\n","        for a in range(self.i_numActions):\n","            for s in range(self.i_numStates):\n","                for ss in range(self.i_numStates):\n","                    self.v_rews[s][a] += np.around(self.v_trp[a][s][ss]*self.v_rews3D[a][s][ss],3)\n","\n","\n","        self.v_rews[15][3] = 0\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kxM1_GZi0ZuH","colab_type":"code","colab":{}},"source":["maze2 = MazeMDP2()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5adnT8FN4cH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1225a046-3d35-468b-a16b-64773710d92e","executionInfo":{"status":"ok","timestamp":1586390083181,"user_tz":240,"elapsed":333,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}}},"source":["maze2.v_trp"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]],\n"," [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n"," [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n"," [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n","  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]]"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"KppLzkLdN7af","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"e9033545-c22b-48d9-b7aa-577b8ec1db28","executionInfo":{"status":"ok","timestamp":1586390093372,"user_tz":240,"elapsed":233,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}}},"source":["maze2.v_actions"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 2],\n"," [1, 2, 3],\n"," [1, 2, 3],\n"," [1, 2, 3],\n"," [0, 1, 2],\n"," [0, 1, 2, 3],\n"," [0, 1, 2, 3],\n"," [0, 1, 3],\n"," [0, 1, 3],\n"," [0, 1, 2, 3],\n"," [0, 1, 2, 3],\n"," [0, 1, 3],\n"," [0, 2],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 3]]"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"JPV0HxjWGOdI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"f915207f-eba2-4f4b-cfe7-e0b10a5decca","executionInfo":{"status":"ok","timestamp":1586390070304,"user_tz":240,"elapsed":256,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}}},"source":["maze2.v_rews"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0, -1, -1, 0],\n"," [-1, -1, -1, -1],\n"," [-1, -1, -1, -1],\n"," [-1, -1, -1, -1],\n"," [0, -1, -1, -1],\n"," [-1, -1, -1, -1],\n"," [-1, -1, -1, -1],\n"," [-1, -1, -1, -1],\n"," [-1, -1, -1, -1],\n"," [-1, -1, -1, -1],\n"," [-1, -1, -1, -1],\n"," [-1, 0, -1, -1],\n"," [-1, -1, -1, -1],\n"," [-1, -1, -1, -1],\n"," [-1, -1, 0, -1],\n"," [-1, 0, -1, 0]]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"CXNCUbp-QlkP","colab_type":"code","colab":{}},"source":["def ValueIteration(giNumStates, gvActions, gvTrp, gvRews, gdDiscountFactor=0.9):\n","    \n","    t_dEpsilon = 0.000001\n","    tmpv_vals = [0]*giNumStates\n","\n","    flat_list = [0,1,2,3] #[item for sublist in gvActions for item in sublist] # flatten the list of lists\n","    t_iNumActions = len(set(flat_list))\n","    \n","    t_iCtr = 0\n","    while True:\n","        \n","        # print(\"t_iCtr\", t_iCtr)\n","        # print(\"tmpv_vals\", tmpv_vals)\n","        \n","        currentv_vals = [0]*giNumStates\n","        t_qValues = [[float(\"-inf\") for a in range(t_iNumActions)] for s in range(giNumStates)]\n","        t_vPol = [float(\"-inf\") for s in range(giNumStates)]\n","        for s in range(giNumStates):\n","            for a in gvActions[s]:\n","                t_qValues[s][a] = gvRews[s][a]\n","                #print(gvRews[s][a])\n","                for ss in range(giNumStates):\n","                    t_qValues[s][a] += gdDiscountFactor*gvTrp[a][s][ss]*tmpv_vals[ss]\n","        \n","        #print(\"t_qValues\", t_qValues)\n","        currentv_vals = np.amax(np.around(t_qValues, 3), axis = 1)\n","        t_vPol = np.argmax(np.around(t_qValues, 3), axis = 1)\n","        #print(\"currentv_vals\", currentv_vals)\n","        if np.linalg.norm(np.array(tmpv_vals) - np.array(currentv_vals)) < t_dEpsilon:\n","            break\n","        \n","        tmpv_vals = currentv_vals\n","        t_iCtr = t_iCtr + 1\n","\n","    return tmpv_vals, t_vPol, t_iCtr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7nd2j9pRa3XY","colab_type":"code","outputId":"79afc013-43dd-4e75-838f-287564f1e77b","executionInfo":{"status":"ok","timestamp":1586390739604,"user_tz":240,"elapsed":288,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["#looking at this solution it looks a little bit naive, always go down until last row then go right.\n","ValueIteration(maze2.i_numStates, maze2.v_actions, maze2.v_trp, maze2.v_rews, gdDiscountFactor=0.9)"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([-4.095, -3.439, -2.71 , -1.9  , -3.439, -2.71 , -1.9  , -1.   ,\n","        -2.71 , -1.9  , -1.   ,  0.   , -1.9  , -1.   ,  0.   ,  0.   ]),\n"," array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3]),\n"," 15)"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"udN3mdbUQYCr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c7b09290-6a6d-4542-fc6b-dde91b7a3990","executionInfo":{"status":"ok","timestamp":1586390753408,"user_tz":240,"elapsed":4276,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}}},"source":["%timeit ValueIteration(maze2.i_numStates, maze2.v_actions, maze2.v_trp, maze2.v_rews, gdDiscountFactor=0.9)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["100 loops, best of 3: 9.27 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M6gHK8r7GgDU","colab_type":"code","colab":{}},"source":["def QLearning(giNumStates, giNumActions, gvActions, gvTrp, gvRews, gdDiscountFactor=0.9, step_size=0.5):\n","    \n","    iter_limit = 100\n","    d_epsilon = 0.05 # randomly select an action with this prob\n","\n","    # episodes of each run\n","    episodes = 100\n","\n","    # perform 50 independent runs\n","    runs = 10\n","    GOAL = 15\n","\n","    rewards_q_learning = np.zeros(episodes)\n","    for r in tqdm(range(runs)):\n","        \n","        print(\"begin-run\", r)\n","\n","        # initialize q-values\n","        q_value = np.array(np.ones((giNumStates,giNumActions)) * -np.inf)\n","        for s in range(giNumStates):\n","            q_value[s, gvActions[s]] = 0 \n","        \n","        for i in range(0, episodes):\n","            state = 0 # start high battery\n","            rewards = 0.0\n","\n","            iter_ctr = 0\n","            while state != GOAL: # for the problems with absorbing/terminal states\n","            #while iter_ctr+1 < iter_limit:\n","\n","                # choose an action based on epsilon greedy algorithm\n","                action = -1\n","                if (random.random() < 0.05):\n","                    action = np.random.choice(gvActions[state])\n","                else:\n","                    values_ = q_value[state, :]\n","                    action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n","                # print(\"iter_ctr-action\", iter_ctr, action)\n","\n","                next_state = np.random.choice(np.arange(len(gvTrp[action][state])), p = gvTrp[action][state])\n","                \n","                reward = gvRews[state][action]\n","                \n","                rewards += reward\n","\n","                # Q-Learning update\n","                q_value[state, action] += step_size * (reward + gdDiscountFactor * np.max(q_value[next_state, :])-q_value[state, action])\n","                #print(\"iter_ctr-action-state-next_state-q_value\", iter_ctr, action, state, next_state, q_value)\n","                \n","                state = next_state\n","                iter_ctr = iter_ctr + 1\n","            \n","            rewards_q_learning[i] += rewards\n","            # print(\"rewards_q_learning\", rewards_q_learning)\n","        print(\"end-run\", r)\n","    \n","    rewards_q_learning /= runs\n","\n","    print(\"rewards_q_learning\", rewards_q_learning)\n","\n","    # draw reward curves\n","    plt.plot(rewards_q_learning, label='Q-Learning')\n","    plt.xlabel('Episodes')\n","    plt.ylabel('Sum of rewards during episode')\n","    plt.ylim([-200, 200])\n","    plt.legend()\n","\n","    plt.savefig('qlearning_v0.png')\n","    plt.close()\n","\n","    # get the policy\n","    optimal_policy = {}\n","    for s in range(giNumStates):\n","        bestAction = np.argmax(q_value[s, :])\n","        optimal_policy[s] = bestAction\n","    \n","    print(\"\\n\\n POLICY\")\n","    for key,val in optimal_policy.items():\n","        print(\"state\", key, \"policy\", val)\n","\n","    print(iter_ctr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xy4GXvwPMWCw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"691b30b8-9d30-4216-89de-b0d9f8326be3","executionInfo":{"status":"ok","timestamp":1586390769846,"user_tz":240,"elapsed":3793,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}}},"source":["#looks pretty good, the policy looks like its moving towards 15\n","%timeit QLearning(maze2.i_numStates, maze2.i_numActions, maze2.v_actions, maze2.v_trp, maze2.v_rews, gdDiscountFactor=0.9, step_size=0.5)"],"execution_count":48,"outputs":[{"output_type":"stream","text":[" 20%|██        | 2/10 [00:00<00:00, 14.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["begin-run 0\n","end-run 0\n","begin-run 1\n","end-run 1\n","begin-run 2\n"],"name":"stdout"},{"output_type":"stream","text":["\r 40%|████      | 4/10 [00:00<00:00, 13.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["end-run 2\n","begin-run 3\n","end-run 3\n","begin-run 4\n","end-run 4\n","begin-run 5\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|████████  | 8/10 [00:00<00:00, 13.41it/s]"],"name":"stderr"},{"output_type":"stream","text":["end-run 5\n","begin-run 6\n","end-run 6\n","begin-run 7\n","end-run 7\n","begin-run 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 13.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["end-run 8\n","begin-run 9\n","end-run 9\n","rewards_q_learning [-38.3 -28.1 -24.7 -17.2 -12.6 -13.8 -12.4  -8.9 -12.4 -10.2  -7.6  -9.2\n","  -8.1  -9.7  -8.1  -8.2  -8.2  -7.2  -8.1  -6.9  -6.3  -6.   -6.   -6.\n","  -7.   -5.8  -6.   -5.6  -6.3  -6.7  -5.4  -6.8  -5.5  -5.7  -5.8  -6.4\n","  -6.5  -5.8  -6.   -6.   -5.6  -5.1  -5.3  -5.8  -5.2  -6.2  -5.3  -5.1\n","  -5.3  -5.   -5.3  -5.4  -5.7  -5.   -5.2  -5.3  -5.3  -5.2  -5.8  -5.5\n","  -5.4  -5.   -5.1  -5.2  -5.6  -5.2  -5.2  -5.   -5.   -5.2  -5.3  -5.2\n","  -5.2  -5.   -5.5  -5.2  -5.3  -5.4  -5.5  -5.2  -5.4  -5.2  -5.3  -5.4\n","  -5.1  -5.2  -5.   -6.   -5.   -5.4  -5.8  -5.4  -5.2  -5.2  -5.2  -5.\n","  -5.2  -5.2  -5.6  -5. ]\n","\n","\n"," POLICY\n","state 0 policy "],"name":"stdout"},{"output_type":"stream","text":[" 20%|██        | 2/10 [00:00<00:00, 13.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["1\n","state 1 policy 1\n","state 2 policy 2\n","state 3 policy 1\n","state 4 policy 2\n","state 5 policy 1\n","state 6 policy 1\n","state 7 policy 1\n","state 8 policy 1\n","state 9 policy 1\n","state 10 policy 1\n","state 11 policy 1\n","state 12 policy 2\n","state 13 policy 2\n","state 14 policy 2\n","state 15 policy 0\n","6\n","begin-run 0\n","end-run 0\n","begin-run 1\n","end-run 1\n","begin-run"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 5/10 [00:00<00:00, 12.44it/s]"],"name":"stderr"},{"output_type":"stream","text":[" 2\n","end-run 2\n","begin-run 3\n","end-run 3\n","begin-run 4\n","end-run 4\n","begin-run 5\n","end-run 5\n","begin-run 6\n"],"name":"stdout"},{"output_type":"stream","text":[" 90%|█████████ | 9/10 [00:00<00:00, 13.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["end-run 6\n","begin-run 7\n","end-run 7\n","begin-run 8\n","end-run 8\n","begin-run 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 12.82it/s]\n","  0%|          | 0/10 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["end-run 9\n","rewards_q_learning [-26.3 -42.  -19.7 -22.5 -13.7 -12.6 -10.9 -10.4  -7.9  -7.6  -8.8  -9.4\n","  -8.3  -9.4  -7.3  -6.8  -6.6  -9.   -7.5  -7.2  -7.7  -6.6  -7.3  -6.2\n","  -5.6  -5.8  -5.7  -6.3  -6.6  -5.5  -5.4  -5.8  -6.   -6.7  -5.8  -5.9\n","  -6.2  -6.6  -6.3  -5.6  -5.7  -5.2  -5.9  -5.3  -5.3  -5.2  -6.2  -5.4\n","  -5.5  -5.2  -5.2  -5.6  -5.2  -5.   -6.5  -5.6  -5.4  -5.6  -5.   -5.4\n","  -5.2  -5.   -5.   -5.4  -5.4  -5.8  -5.4  -5.4  -5.5  -5.5  -5.4  -5.6\n","  -5.   -5.3  -5.   -5.   -5.8  -5.6  -5.   -5.2  -5.4  -5.2  -5.   -5.\n","  -5.   -5.2  -5.   -5.   -5.2  -5.3  -5.7  -5.   -5.   -5.1  -5.2  -5.2\n","  -5.2  -5.1  -5.7  -5. ]\n","\n","\n"," POLICY\n","state 0 policy 1\n","state 1 policy 1\n","state 2 policy 1\n","state 3 policy 1\n","state 4 policy 2\n","state 5 policy 1\n","state 6 policy 1\n","state 7 policy 1\n","state 8 policy 1\n","state 9 policy 1\n","state 10 policy 1\n","state 11 policy 1\n","state 12 policy 2\n","state 13 policy 2\n","state 14 policy 2\n","state 15 policy 0\n","6\n","begin-run 0\n","end-run "],"name":"stdout"},{"output_type":"stream","text":[" 40%|████      | 4/10 [00:00<00:00, 14.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["0\n","begin-run 1\n","end-run 1\n","begin-run 2\n","end-run 2\n","begin-run 3\n","end-run 3\n","begin-run 4\n"],"name":"stdout"},{"output_type":"stream","text":["\r 60%|██████    | 6/10 [00:00<00:00, 13.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["end-run 4\n","begin-run 5\n","end-run 5\n","begin-run 6\n","end-run 6\n","begin-run 7\n","end-run "],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 14.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["7\n","begin-run 8\n","end-run 8\n","begin-run 9\n","end-run 9\n","rewards_q_learning [-44.2 -24.5 -20.5 -19.5 -15.9 -13.6  -9.5  -9.2 -10.2  -9.2  -7.2  -8.2\n","  -7.7  -9.2  -7.6  -9.   -9.1  -7.2  -7.5  -7.7  -5.9  -7.7  -6.1  -5.7\n","  -6.1  -5.8  -6.   -6.4  -7.   -5.9  -5.4  -5.9  -5.9  -5.5  -6.3  -5.6\n","  -5.8  -6.4  -5.9  -6.3  -5.8  -5.9  -5.5  -5.2  -6.4  -5.   -5.9  -5.3\n","  -5.6  -5.   -5.2  -5.4  -5.3  -5.6  -5.   -5.3  -5.   -5.   -5.   -6.\n","  -5.4  -5.8  -5.2  -5.6  -5.4  -5.6  -5.4  -5.2  -5.9  -5.6  -5.4  -5.\n","  -5.4  -5.   -5.2  -5.2  -5.4  -5.   -5.   -5.2  -5.   -5.4  -5.   -5.4\n","  -5.4  -5.   -5.7  -5.6  -5.4  -5.3  -5.2  -5.2  -5.6  -5.6  -5.2  -5.\n","  -5.   -5.2  -5.4  -5.2]\n"],"name":"stdout"},{"output_type":"stream","text":[" 20%|██        | 2/10 [00:00<00:00, 14.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," POLICY\n","state 0 policy 2\n","state 1 policy 2\n","state 2 policy 1\n","state 3 policy 1\n","state 4 policy 1\n","state 5 policy 1\n","state 6 policy 1\n","state 7 policy 1\n","state 8 policy 1\n","state 9 policy 1\n","state 10 policy 2\n","state 11 policy 1\n","state 12 policy 2\n","state 13 policy 2\n","state 14 policy 2\n","state 15 policy 0\n","6\n","begin-run 0\n","end-run 0\n","begin-run 1\n","end-run 1\n","begin-run 2\n"],"name":"stdout"},{"output_type":"stream","text":["\r 40%|████      | 4/10 [00:00<00:00, 14.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["end-run 2\n","begin-run 3\n","end-run 3\n","begin-run 4\n","end-run 4\n","begin-run 5\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|████████  | 8/10 [00:00<00:00, 13.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["end-run 5\n","begin-run 6\n","end-run 6\n","begin-run 7\n","end-run 7\n","begin-run 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 13.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["end-run 8\n","begin-run 9\n","end-run 9\n","rewards_q_learning [-33.9 -31.5 -25.3 -14.3 -16.9 -16.4  -9.5  -9.3  -8.9  -7.   -8.9  -8.2\n","  -9.3 -10.2  -7.7  -8.3  -8.1  -6.9  -8.4  -6.8  -7.   -6.   -6.   -6.4\n","  -5.9  -5.9  -6.   -6.4  -5.9  -5.8  -5.8  -6.2  -6.2  -5.6  -6.   -5.6\n","  -6.9  -6.   -6.   -5.9  -5.7  -6.4  -5.7  -5.3  -5.2  -5.2  -5.1  -5.2\n","  -5.1  -5.8  -5.5  -5.6  -5.2  -5.   -5.2  -5.   -5.2  -5.2  -5.   -5.5\n","  -5.4  -5.5  -5.2  -5.2  -5.2  -5.7  -5.4  -5.4  -5.   -5.6  -5.   -5.1\n","  -5.2  -5.2  -5.5  -5.4  -5.   -5.2  -5.6  -5.8  -5.2  -5.2  -5.   -5.4\n","  -5.6  -5.   -5.8  -5.6  -5.6  -5.3  -5.6  -5.4  -5.4  -5.2  -5.2  -5.2\n","  -5.2  -5.   -6.2  -5. ]\n","\n","\n"," POLICY\n","state 0 policy 1\n","state 1 policy 1\n","state 2 policy 2\n","state 3 policy 1\n","state 4 policy 2\n","state 5 policy 1\n","state 6 policy 1\n","state 7 policy 1\n","state 8 policy 1\n","state 9 policy 2\n","state 10 policy 1\n","state 11 policy 1\n","state 12 policy 2\n","state 13 policy 2\n","state 14 policy 2\n","state 15 policy 0\n","6\n","1 loop, best of 3: 846 ms per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lvtYGQ45OTvy","colab_type":"text"},"source":["QLearn takes longer since its running 10 times of a while loop\n","%timeit function tells us it took our Qlearn Function 846ms (best time) vs. 9.63ms in our VI. Both their policy are quite different but are moving towards the state 15 (our GOAL State)\n"]}]}